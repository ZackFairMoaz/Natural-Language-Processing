{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers Overview\n",
    "#### Tokenizers serve one purpose: to translate text into data that can be processed by the model (numbers/ID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Based\n",
    "Each word has a specific Id (words are seperated by relevant punctuations)\n",
    "\n",
    "The Two Limitations are:\n",
    "###### 1) Similar words having different number (Cat vs Cats) 2) Very large vocab size due to every single word having a token (Can be resolved by using top k used words, remaining converted to oov (results in lost info) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Based\n",
    "Character-based tokenizers split the text into characters, rather than words.\n",
    "\n",
    "The Two primary Benefits compared to Word Based are:\n",
    "###### 1) The vocabulary is much smaller. 2) Much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
    "\n",
    "The Two Limitations are:\n",
    "###### 1) it’s less meaningful: each character doesn’t mean a lot on its own, whereas that is the case with words 2) We’ll end up with a very large amount of tokens to be processed by our model 3)  Questions arise concerning spaces and punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword tokenization\n",
    "Frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. (Annoying - ly / Postive - ly)\n",
    "\n",
    "This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens\n",
    "\n",
    "The Two primary Benefits compared to Word Based are:\n",
    "###### 1) Semantic Meaning is Kept inteact Better. 2) Space Efficient (2 Tokens for long words)\n",
    "\n",
    "More Techniques :\n",
    "\n",
    "###### Byte-level = GPT-2, WordPiece = BERT, SentencePiece/Unigram (several Multilingual Models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Saved_Tokenizers\\\\tokenizer_config.json',\n",
       " 'Saved_Tokenizers\\\\special_tokens_map.json',\n",
       " 'Saved_Tokenizers\\\\vocab.txt',\n",
       " 'Saved_Tokenizers\\\\added_tokens.json',\n",
       " 'Saved_Tokenizers\\\\tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"Saved_Tokenizers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Translating text to numbers is known as encoding. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)\n",
    "# ### used as placeholdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Tokens to Input ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode() method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a Transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
